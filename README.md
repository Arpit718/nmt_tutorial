This repository is a collection of items I created towards the completion of my senior thesis in the Applied Mathematics Dept. at Loyola Marymount Univeristy under the supervision of [Dr. Thomas Laurent](http://thomaslaurent.lmu.build/homepage.html).
The thesis revolved around creating a tutorial on Neural Machine Translation. The tutorial was largely inspired and drew from the PyTorch tutorial ["Translation with a Sequence to Sequence Network and Attention"](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html "PyTorch Tutorial). However, this tutorial was enhanced in a number of ways. Most notably, this code allows the data to be processed in mini-batches, adds the option to easily split the dataset into train and test data, and implements a number of other useful and effective enhancements (optional learning rate schedules, ability to handle datasets in different formats, ability to handle unknown words, etc.).
Along with enhancing the code in the PyTorch tutorial, I completed a Thesis paper which details the math behind the Encoder Decoder neural network. Furthermore I condensed all of my work into a Medium article. The article is about a 20 minute read but gives a thorough explanation of the Encoder Decoder structure along with walking through the various parts of the code.
